Hey team,
Over the weekend, I had some thoughts on improving our feature validation process. At the feature validation stage, I’m already leveraging dynamic prompt engineering, which generates rules based on dataset metadata. Moving forward, I think we could alternate between prompt-based rule generation and fine-tuning, depending on dataset complexity, to see which method performs best.
One idea is to fine-tune the validation models for specific domains (may be insurance ), making rule generation more accurate and context-aware. To do this, we’d need to gather a dataset of historical validation rules and data issues—logged validation outcomes, manual overrides, and associated metadata and feature distributions. By training the model on this, it’ll adapt better to domain-specific validation scenarios.
I’m also considering integrating RAG into the pipeline to pull existing rules from our knowledge base before generating new ones. This will ensure we’re building on past knowledge and only creating new rules when necessary, making the process more efficient and adaptive (though we still need to think more about the structure of the knowledge base—how it will be built, accessed, and updated).
Another idea is around the drift detection, which I’d like to incorporate during the validation process or when new data is introduced, or perhaps build a model for it—not sure yet. We can use tests like K-S or PSI to monitor distribution changes and, if drift is detected, dynamically regenerate validation rules to keep them relevant as the data evolves.
Lastly, I plan to improve our custom validation code blocks. Instead of generating these ad-hoc, we could store and update reusable snippets using RAG, ensuring they evolve along with the validation rules. This would streamline the entire process and make it more scalable over time.
Let me know your thoughts or if you have any additional ideas!
